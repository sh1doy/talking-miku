{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports(default)\n",
    "import sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.optimizers import *\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "keras = tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_files = glob(\"../dataset/charactor/*.parsed\")\n",
    "conv_files = glob(\"../dataset/conversation/nucc_*.parsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    with open(file, \"r\") as f:\n",
    "        res = f.readlines()\n",
    "        res = [line.replace(\"\\n\", \"\") for line in res]\n",
    "    return res\n",
    "char_texts = [read_file(file) for file in char_files]\n",
    "conv_texts = [read_file(file) for file in conv_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "for text in char_texts + conv_texts:\n",
    "    for line in text:\n",
    "        vocab.update(set(line.split(\" \")))\n",
    "\n",
    "word2id = {}\n",
    "id2word = {}\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, word2id, id2word):\n",
    "        self.word2id = word2id\n",
    "        self.id2word = id2word\n",
    "    \n",
    "    def encode(self, seq):\n",
    "        return [1] + [self.word2id[word] for word in seq] + [2]\n",
    "    \n",
    "    def decode(self, seq):\n",
    "        return \"\".join([self.id2word[word] for word in seq]).replace(\"▁\", \"\")[3:-3]\n",
    "\n",
    "for e, word in enumerate([\"<BOS>\", \"<EOS>\", \"<UNK>\"] + sorted(list(vocab))):\n",
    "    word2id[word] = e\n",
    "    id2word[e] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "id2word = {}\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, word2id, id2word):\n",
    "        self.word2id = word2id\n",
    "        self.id2word = id2word\n",
    "    \n",
    "    def encode(self, seq):\n",
    "        return [1] + [self.word2id[word] for word in seq] + [2]\n",
    "    \n",
    "    def decode(self, seq):\n",
    "        return \"\".join([self.id2word[word] for word in seq]).replace(\"▁\", \"\")[3:-3]\n",
    "\n",
    "for e, word in enumerate([\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"] + sorted(list(vocab))):\n",
    "    word2id[word] = e\n",
    "    id2word[e] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(word2id, id2word)\n",
    "char_seqs = [[tokenizer.encode(line.split(\" \")) for line in text] for text in char_texts]\n",
    "conv_seqs = [[tokenizer.encode(line.split(\" \")) for line in text] for text in conv_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_x = []\n",
    "conv_y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in conv_seqs:\n",
    "    conv_x.append([1, 2])\n",
    "    for line in text:\n",
    "        conv_x.append(line)\n",
    "        conv_y.append(line)\n",
    "    conv_x.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 200\n",
    "NUM_UNITS = 400\n",
    "SEQ_LEN = 150\n",
    "BEAM_WIDTH = 3\n",
    "BATCH_SIZE = 256\n",
    "VOCAB = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(x, y, seq_len):\n",
    "    x = pad_sequences(x, seq_len, padding=\"post\", truncating=\"post\")\n",
    "    y_in = pad_sequences([line[:-1] for line in y], seq_len, padding=\"post\", truncating=\"post\")\n",
    "    y_out = pad_sequences([line[1:] for line in y], seq_len, padding=\"post\", truncating=\"post\")\n",
    "    return [x, y_in], y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq:\n",
    "    def __init__(self):\n",
    "        encoder_inputs = Input([None], dtype=\"int32\", name=\"x\")\n",
    "        E_embed = Embedding(VOCAB, EMBEDDING_SIZE, mask_zero=True, name=\"E_embed\")(encoder_inputs)\n",
    "        encoder1 = LSTM(NUM_UNITS, return_state=True, return_sequences=True, dropout=.2, recurrent_dropout=.2)\n",
    "        encoder2 = LSTM(NUM_UNITS, return_state=True, dropout=.2, recurrent_dropout=.2)\n",
    "        out, *mid_states1 = encoder1(E_embed)\n",
    "        out, *mid_states2 = encoder2(out)\n",
    "        # End2end learning\n",
    "        decoder_inputs = Input(shape=[None], dtype=\"int32\", name=\"y_\")\n",
    "        F_embed = Embedding(VOCAB, EMBEDDING_SIZE, mask_zero=True, name=\"F_embed\")(decoder_inputs)\n",
    "        decoder1 = LSTM(NUM_UNITS, return_sequences=True, return_state=True, dropout=.2, recurrent_dropout=.2)\n",
    "        decoder2 = LSTM(NUM_UNITS, return_sequences=True, return_state=True, dropout=.2, recurrent_dropout=.2)\n",
    "        decoder_outputs, *decoder_states1 = decoder1(F_embed, initial_state=mid_states1)\n",
    "        decoder_outputs, *decoder_states2 = decoder2(decoder_outputs, initial_state=mid_states2)\n",
    "        decoder_dense = Dense(VOCAB, activation='softmax', name=\"output_dense\")\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "        self.training_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "        # Single Encoder\n",
    "        self.encoder_model = Model(inputs=encoder_inputs, outputs=mid_states1 + mid_states2)\n",
    "        # Single Decoder \n",
    "        decoder_states = [Input([NUM_UNITS]) for _ in range(4)]\n",
    "        d_out, *new_decoder_states1 = decoder1(F_embed, initial_state=decoder_states[0:2])\n",
    "        d_out, *new_decoder_states2 = decoder2(d_out, initial_state=decoder_states[2:4])\n",
    "        new_decoder_outputs = decoder_dense(d_out)\n",
    "\n",
    "        self.decoder_model = Model(inputs=[decoder_inputs] + decoder_states,\n",
    "                              outputs=[new_decoder_outputs] + new_decoder_states1 + new_decoder_states2)\n",
    "        \n",
    "        self.training_model.compile(Adam(1e-3), loss='sparse_categorical_crossentropy')\n",
    "#         self.encoder_model.compile(Adam(1e-4), loss='sparse_categorical_crossentropy')\n",
    "#         self.decoder_model.compile(Adam(1e-4), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "    # generate target given source sequence\n",
    "    def predict_sequence(self, source, n_steps, mode=\"greedy\"):\n",
    "        # encode\n",
    "        state = self.encoder_model.predict(source)\n",
    "        # start of sequence input\n",
    "        x = np.array([[1] for _ in range(len(source))])\n",
    "        # collect predictions\n",
    "        output = list()\n",
    "        for t in range(n_steps):\n",
    "            # predict next char\n",
    "            x, *state = self.decoder_model.predict([x] + state)\n",
    "            if mode==\"greedy\":\n",
    "                x = x.argmax(-1)\n",
    "            # store prediction\n",
    "            output.append(x)\n",
    "            # update target sequence\n",
    "        return np.concatenate(output, -1)\n",
    "\n",
    "model = Seq2seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = get_batch(conv_x, conv_y, SEQ_LEN)\n",
    "model.training_model.fit(x, y[:,:,np.newaxis], 64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.training_model.save_weights(\"models/conv.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = conv_x[10:12]\n",
    "inputs = pad_sequences(source, 150, padding=\"post\", truncating=\"post\")\n",
    "states = model.predict_sequence(inputs, 50)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
